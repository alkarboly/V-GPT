# VGPT - Vector Database Search for Custom Datasets

VGPT provides a suite of tools for crawling data sources, creating vector embeddings, and searching through your custom data using natural language.

## Overview

The project consists of three main components:

1. **Crawler**: Ethically crawls data sources to collect metadata, formats the data as both JSON and Markdown.

2. **Embedder**: Processes the crawled data, generates text embeddings using OpenAI's API, and stores them in a Pinecone vector database.

3. **Searcher**: Provides a semantic search interface using both the vector database and LLMs to answer natural language queries about your data.

## Requirements

- Python 3.7+
- OpenAI API key
- Pinecone API key
- Required packages (install with `pip install -r requirements.txt`)

## Installation

1. Clone this repository:
   ```
   git clone https://github.com/yourusername/VGPT.git
   cd VGPT
   ```

2. Create a virtual environment (recommended):
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Create a `.env` file in the project root with your API keys and configuration:
   ```
   # Copy the example file
   cp .env.example .env
   
   # Now edit the .env file with your API keys and configuration
   ```

## Configuration

All configuration parameters can be set in the `.env` file. Key parameters include:

- **API Keys**: `OPENAI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_ENVIRONMENT`
- **Data Source**: `DATA_SOURCE_URL`, `USER_AGENT` 
- **Crawler Settings**: Rate limits, cache settings, etc.
- **Directory Settings**: Where to store crawled data and cache
- **Vector Database Settings**: Index name, namespace, embedding model, etc.
- **Search Settings**: Chat model, number of results, etc.

See `.env.example` for a complete list of configuration options.

## Usage

### 1. Crawl the Data Source

```bash
python -m vgpt.crawler [--pages N] [--output-dir PATH]
```

Options:
- `--pages`: Number of pages to crawl (default from .env)
- `--output-dir`: Directory to save crawled data (default from .env)
- `--datasets-per-page`: Maximum datasets to process per page
- `--no-cache`: Disable request caching

#### Using the Batch Runner

For large crawls, the batch runner allows you to process multiple batches with pauses between them:

```bash
python tests/crawler.py batch-runner [start-end] [batch_size] [pause_seconds]
```

Options:
- `start-end`: Range of batches to process (e.g., "1-3" for batches 1, 2, and 3)
- `batch_size`: Number of pages per batch (default: 10)
- `pause_seconds`: Pause duration between batches (default: 10 seconds)

Example to process batches 1 through 5 with 15-second pauses:
```bash
python tests/crawler.py batch-runner 1-5 10 15
```

#### Direct Page/Resource Processing

- Process a specific page range:
  ```bash
  python tests/crawler.py 1-10
  ```

- Process a specific dataset/resource URL:
  ```bash
  python tests/crawler.py https://data.virginia.gov/dataset/example-dataset
  ```

#### Logging and Resumability

The crawler implements comprehensive logging for tracking progress and resuming interrupted crawls:

- **Log Files**:
  - `crawler.log`: Main log file with detailed activity logging
  - `data/datasets/all_dataset_urls.txt`: Master list of all discovered dataset URLs
  - `data/datasets/dataset_urls_batch_{start}-{end}.txt`: URLs from each processed batch

- **Progress Tracking**:
  - Each batch logs statistics on completion (datasets processed, successful saves, preview tables)
  - The system tracks when column information is successfully extracted
  - Error logging includes detailed tracebacks for debugging

- **Resumability**:
  - To resume an interrupted crawl, check the last batch processed in logs or URL files
  - Run the batch runner with a new start parameter picking up where you left off
  - Example: `python tests/crawler.py batch-runner 4-10` to continue from batch 4

- **Cache System**:
  - The crawler uses persistent caching (in `data/cache/`)
  - Cached requests are reused across runs, speeding up resumed crawls
  - Significantly reduces server load during multi-session crawling

The crawler will:
- Respect server resources with proper rate limiting and caching
- Extract detailed metadata about each dataset
- Identify column names and data types when available
- Save results as both JSON and Markdown files

### 2. Generate and Store Vector Embeddings

```bash
python -m vgpt.embedder [--data-dir PATH] [--index-name NAME]
```

Options:
- `--data-dir`: Directory containing dataset files (default from .env)
- `--index-name`: Name of the vector index (default from .env)
- `--batch-size`: Batch size for vector database uploads
- `--embedding-model`: OpenAI embedding model to use
- `--namespace`: Vector database namespace

The embedder will:
- Read the Markdown files generated by the crawler
- Chunk the text appropriately for embedding
- Generate embeddings using OpenAI's API
- Store the embeddings in a vector database with metadata

### 3. Search Data with Semantic Queries

```bash
python -m vgpt.searcher [--index-name NAME] [--query "your query here"]
```

Options:
- `--index-name`: Name of the vector index (default from .env)
- `--namespace`: Vector database namespace (default from .env)
- `--embedding-model`: OpenAI embedding model (default from .env)
- `--chat-model`: OpenAI chat model (default from .env)
- `--top-k`: Number of results to return (default from .env)
- `--query`: Run a single query (non-interactive mode)

If no query is provided, the search tool runs in interactive mode, allowing you to:
- Enter natural language queries
- View search results in a formatted table
- Get AI-generated responses based on the search results
- Explore detailed information about each result

## Why Markdown?

The crawler saves dataset information as both JSON and Markdown files. The Markdown format is used for embedding because:

1. **Structure**: Markdown provides enough structure to distinguish headers, sections, and formatting.
2. **Readability**: It preserves the human-readable aspect of the data.
3. **Embedding Quality**: Text processing models understand Markdown format well.
4. **Simplicity**: It's a lightweight format that maintains important semantic information.

## Project Structure

```
VGPT/
├── vgpt/                    # Main package
│   ├── __init__.py          # Package initialization
│   ├── crawler.py           # Dataset crawler
│   ├── embedder.py          # Embedding generation and storage
│   └── searcher.py          # Search interface
├── .env                     # Environment variables (API keys)
├── .env.example             # Example environment file
├── requirements.txt         # Project dependencies
├── README.md                # This file
└── data/                    # Data directory
    ├── crawled_datasets/    # Crawler output
    └── cache/               # Request cache
```

## Notes on API Usage

This project uses external APIs that require API keys and may have usage limits or costs:
- **OpenAI API**: For generating embeddings and powering the search interface
- **Pinecone API**: For storing and querying vector embeddings

Please review the pricing and limits of these services before heavy usage.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details. 